{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "671a34a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06d8119c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus for testing\n",
    "corpus = '''\n",
    "    Object raspberrypi functools dict kwargs. Gevent raspberrypi functools. Dunder raspberrypi decorator dict didn't lambda zip import pyramid, she lambda iterate?\n",
    "    Kwargs raspberrypi diversity unit object gevent. Import fall integration decorator unit django yield functools twisted. Dunder integration decorator he she future.\n",
    "    Python raspberrypi community pypy. \n",
    "    Kwargs integration beautiful test reduce gil python closure. Gevent he integration generator fall test kwargs raise didn't visor he itertools...\n",
    "    Reduce integration coroutine bdfl he python. Cython didn't integration while beautiful list python didn't nit!\n",
    "    Object fall diversity 2to3 dunder script. Python fall for: integration exception dict kwargs dunder pycon. Import raspberrypi beautiful test import six web. Future \n",
    "    integration mercurial self script web. Return raspberrypi community test she stable.\n",
    "    Django raspberrypi mercurial unit import yield raspberrypi visual rocksdahouse. Dunder raspberrypi mercurial list reduce class test scipy helmet zip?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e361d795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Object raspberrypi functools dict kwargs. Gevent raspberrypi functools. Dunder raspberrypi decorator dict didn't lambda zip import pyramid, she lambda iterate?\n",
      "    Kwargs raspberrypi diversity unit object gevent. Import fall integration decorator unit django yield functools twisted. Dunder integration decorator he she future.\n",
      "    Python raspberrypi community pypy. \n",
      "    Kwargs integration beautiful test reduce gil python closure. Gevent he integration generator fall test kwargs raise didn't visor he itertools...\n",
      "    Reduce integration coroutine bdfl he python. Cython didn't integration while beautiful list python didn't nit!\n",
      "    Object fall diversity 2to3 dunder script. Python fall for: integration exception dict kwargs dunder pycon. Import raspberrypi beautiful test import six web. Future \n",
      "    integration mercurial self script web. Return raspberrypi community test she stable.\n",
      "    Django raspberrypi mercurial unit import yield raspberrypi visual rocksdahouse. Dunder raspberrypi mercurial list reduce class test scipy helmet zip?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ed7398",
   "metadata": {},
   "source": [
    "## Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65274f34",
   "metadata": {},
   "source": [
    "The normalization step involves some general cleanup, such as removing needless whitespace, lowercasing, and/or removing accents. If you’re familiar with Unicode normalization (such as NFC or NFKC), this is also something the tokenizer may apply.\n",
    "\n",
    "The Transformers tokenizer has an attribute called backend_tokenizer that provides access to the underlying tokenizer from the Tokenizers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0acd0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'object raspberrypi functools dict kwargs gevent raspberrypi functools dunder raspberrypi decorator dict didnt lambda zip import pyramid she lambda iterate\\n    kwargs raspberrypi diversity unit object gevent import fall integration decorator unit django yield functools twisted dunder integration decorator he she future\\n    python raspberrypi community pypy \\n    kwargs integration beautiful test reduce gil python closure gevent he integration generator fall test kwargs raise didnt visor he itertools\\n    reduce integration coroutine bdfl he python cython didnt integration while beautiful list python didnt nit\\n    object fall diversity to dunder script python fall for integration exception dict kwargs dunder pycon import raspberrypi beautiful test import six web future \\n    integration mercurial self script web return raspberrypi community test she stable\\n    django raspberrypi mercurial unit import yield raspberrypi visual rocksdahouse dunder raspberrypi mercurial list reduce class test scipy helmet zip'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import regex\n",
    "import re\n",
    "\n",
    "# convert to lower case\n",
    "corpus = corpus.lower()\n",
    " \n",
    "# remove numbers\n",
    "corpus = re.sub(r'\\d+','',corpus)\n",
    " \n",
    "# remove all punctuation except words and space\n",
    "corpus = re.sub(r'[^\\w\\s]','', corpus) \n",
    " \n",
    "# remove white spaces\n",
    "corpus = corpus.strip()\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093179dd",
   "metadata": {},
   "source": [
    "## Pre-tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5b9eb0",
   "metadata": {},
   "source": [
    "A tokenizer cannot be trained on raw text alone. Instead, we first need to split the texts into small entities, like words. That’s where the pre-tokenization step comes in. A word-based tokenizer can simply split a raw text into words on whitespace and punctuation. Those words will be the boundaries of the subtokens the tokenizer can learn during its training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b70d1c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "735699cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'object': 3, 'raspberrypi': 10, 'functools': 3, 'dict': 3, 'kwargs': 5, 'gevent': 3, 'dunder': 5, 'decorator': 3, 'didnt': 4, 'lambda': 2, 'zip': 2, 'import': 5, 'pyramid': 1, 'she': 3, 'iterate': 1, 'diversity': 2, 'unit': 3, 'fall': 4, 'integration': 8, 'django': 2, 'yield': 2, 'twisted': 1, 'he': 4, 'future': 2, 'python': 5, 'community': 2, 'pypy': 1, 'beautiful': 3, 'test': 5, 'reduce': 3, 'gil': 1, 'closure': 1, 'generator': 1, 'raise': 1, 'visor': 1, 'itertools': 1, 'coroutine': 1, 'bdfl': 1, 'cython': 1, 'while': 1, 'list': 2, 'nit': 1, 'to': 1, 'script': 2, 'for': 1, 'exception': 1, 'pycon': 1, 'six': 1, 'web': 2, 'mercurial': 3, 'self': 1, 'return': 1, 'stable': 1, 'visual': 1, 'rocksdahouse': 1, 'class': 1, 'scipy': 1, 'helmet': 1})\n"
     ]
    }
   ],
   "source": [
    "word_freqs = defaultdict(int)\n",
    "words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(corpus.strip())\n",
    "new_words = [word for word, offset in words_with_offsets]\n",
    "for word in new_words:\n",
    "    word_freqs[word] += 1\n",
    "print(word_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54d1b2b",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65d85dc",
   "metadata": {},
   "source": [
    "Like BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet. Since it identifies subwords by adding a prefix (like ## for BERT), each word is initially split by adding that prefix to all the characters inside the word. So, the result look like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3269446f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##j', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##x', '##y', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "alphabet = []\n",
    "for word in word_freqs.keys():\n",
    "    if word[0] not in alphabet:\n",
    "        alphabet.append(word[0])\n",
    "    for letter in word[1:]:\n",
    "        if f\"##{letter}\" not in alphabet:\n",
    "            alphabet.append(f\"##{letter}\")\n",
    "\n",
    "alphabet.sort()\n",
    "alphabet\n",
    "\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fb96ab",
   "metadata": {},
   "source": [
    "Thus, the initial alphabet contains all the characters present at the beginning of a word and the characters present inside a word preceded by the WordPiece prefix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3828489",
   "metadata": {},
   "source": [
    "We also add the special tokens used by the model at the beginning of that vocabulary. In the case of BERT, it’s the list [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "335bc81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##j', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##x', '##y', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c630c1",
   "metadata": {},
   "source": [
    "This list vocab will be our output at the end after adding all the commun pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db52f83",
   "metadata": {},
   "source": [
    "Next we need to split each word, with all the letters that are not the first prefixed by ##:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2407ab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {\n",
    "    word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
    "    for word in word_freqs.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f4cfb90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'object': ['o', '##b', '##j', '##e', '##c', '##t'], 'raspberrypi': ['r', '##a', '##s', '##p', '##b', '##e', '##r', '##r', '##y', '##p', '##i'], 'functools': ['f', '##u', '##n', '##c', '##t', '##o', '##o', '##l', '##s'], 'dict': ['d', '##i', '##c', '##t'], 'kwargs': ['k', '##w', '##a', '##r', '##g', '##s'], 'gevent': ['g', '##e', '##v', '##e', '##n', '##t'], 'dunder': ['d', '##u', '##n', '##d', '##e', '##r'], 'decorator': ['d', '##e', '##c', '##o', '##r', '##a', '##t', '##o', '##r'], 'didnt': ['d', '##i', '##d', '##n', '##t'], 'lambda': ['l', '##a', '##m', '##b', '##d', '##a'], 'zip': ['z', '##i', '##p'], 'import': ['i', '##m', '##p', '##o', '##r', '##t'], 'pyramid': ['p', '##y', '##r', '##a', '##m', '##i', '##d'], 'she': ['s', '##h', '##e'], 'iterate': ['i', '##t', '##e', '##r', '##a', '##t', '##e'], 'diversity': ['d', '##i', '##v', '##e', '##r', '##s', '##i', '##t', '##y'], 'unit': ['u', '##n', '##i', '##t'], 'fall': ['f', '##a', '##l', '##l'], 'integration': ['i', '##n', '##t', '##e', '##g', '##r', '##a', '##t', '##i', '##o', '##n'], 'django': ['d', '##j', '##a', '##n', '##g', '##o'], 'yield': ['y', '##i', '##e', '##l', '##d'], 'twisted': ['t', '##w', '##i', '##s', '##t', '##e', '##d'], 'he': ['h', '##e'], 'future': ['f', '##u', '##t', '##u', '##r', '##e'], 'python': ['p', '##y', '##t', '##h', '##o', '##n'], 'community': ['c', '##o', '##m', '##m', '##u', '##n', '##i', '##t', '##y'], 'pypy': ['p', '##y', '##p', '##y'], 'beautiful': ['b', '##e', '##a', '##u', '##t', '##i', '##f', '##u', '##l'], 'test': ['t', '##e', '##s', '##t'], 'reduce': ['r', '##e', '##d', '##u', '##c', '##e'], 'gil': ['g', '##i', '##l'], 'closure': ['c', '##l', '##o', '##s', '##u', '##r', '##e'], 'generator': ['g', '##e', '##n', '##e', '##r', '##a', '##t', '##o', '##r'], 'raise': ['r', '##a', '##i', '##s', '##e'], 'visor': ['v', '##i', '##s', '##o', '##r'], 'itertools': ['i', '##t', '##e', '##r', '##t', '##o', '##o', '##l', '##s'], 'coroutine': ['c', '##o', '##r', '##o', '##u', '##t', '##i', '##n', '##e'], 'bdfl': ['b', '##d', '##f', '##l'], 'cython': ['c', '##y', '##t', '##h', '##o', '##n'], 'while': ['w', '##h', '##i', '##l', '##e'], 'list': ['l', '##i', '##s', '##t'], 'nit': ['n', '##i', '##t'], 'to': ['t', '##o'], 'script': ['s', '##c', '##r', '##i', '##p', '##t'], 'for': ['f', '##o', '##r'], 'exception': ['e', '##x', '##c', '##e', '##p', '##t', '##i', '##o', '##n'], 'pycon': ['p', '##y', '##c', '##o', '##n'], 'six': ['s', '##i', '##x'], 'web': ['w', '##e', '##b'], 'mercurial': ['m', '##e', '##r', '##c', '##u', '##r', '##i', '##a', '##l'], 'self': ['s', '##e', '##l', '##f'], 'return': ['r', '##e', '##t', '##u', '##r', '##n'], 'stable': ['s', '##t', '##a', '##b', '##l', '##e'], 'visual': ['v', '##i', '##s', '##u', '##a', '##l'], 'rocksdahouse': ['r', '##o', '##c', '##k', '##s', '##d', '##a', '##h', '##o', '##u', '##s', '##e'], 'class': ['c', '##l', '##a', '##s', '##s'], 'scipy': ['s', '##c', '##i', '##p', '##y'], 'helmet': ['h', '##e', '##l', '##m', '##e', '##t']}\n"
     ]
    }
   ],
   "source": [
    "print(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1701dff5",
   "metadata": {},
   "source": [
    "Like BPE, WordPiece learns merge rules. The main difference is the way the pair to be merged is selected. Instead of selecting the most frequent pair, WordPiece computes a score for each pair, using the following formula:\n",
    "$$\n",
    "     score=(freq\\_of\\_pair)/(freq\\_of\\_first\\_element × freq\\_of\\_second\\_element)\n",
    "$$\n",
    "By dividing the frequency of the pair by the product of the frequencies of each of its parts, the algorithm prioritizes the merging of pairs where the individual parts are less frequent in the vocabulary.\n",
    "\n",
    "Now that we are ready for training, let’s write a function that computes the score of each pair. We’ll need to use this at each step of the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7226196",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that computes the scores for each pair of successive elements in each word\n",
    "def compute_pair_scores(splits):\n",
    "    letter_freqs = defaultdict(int)\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            letter_freqs[split[0]] += freq\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            letter_freqs[split[i]] += freq\n",
    "            pair_freqs[pair] += freq\n",
    "        letter_freqs[split[-1]] += freq\n",
    "\n",
    "    scores = {\n",
    "        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
    "        for pair, freq in pair_freqs.items()\n",
    "    }\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "451b6f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('o', '##b'): 0.05555555555555555\n",
      "('##b', '##j'): 0.03333333333333333\n",
      "('##j', '##e'): 0.007142857142857143\n",
      "('##e', '##c'): 0.002976190476190476\n",
      "('##c', '##t'): 0.004746835443037975\n",
      "('r', '##a'): 0.01375\n"
     ]
    }
   ],
   "source": [
    "#Example of score computing\n",
    "pair_scores = compute_pair_scores(splits)\n",
    "for i, key in enumerate(pair_scores.keys()):\n",
    "    print(f\"{key}: {pair_scores[key]}\")\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cce92c",
   "metadata": {},
   "source": [
    "Now, finding the pair with the best score only takes a quick loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb5ea935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('e', '##x') 0.5\n"
     ]
    }
   ],
   "source": [
    "best_pair = \"\"\n",
    "max_score = None\n",
    "for pair, score in pair_scores.items():\n",
    "    if max_score is None or max_score < score:\n",
    "        best_pair = pair\n",
    "        max_score = score\n",
    "\n",
    "print(best_pair, max_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f842d08e",
   "metadata": {},
   "source": [
    "So the first merge to learn is ('e', '##x') -> 'ex', and we add 'ex' to the vocabulary dic that we've created before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9788f1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.append(\"ex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e4d3c8",
   "metadata": {},
   "source": [
    "To continue, we need to apply that merge in our splits dictionary. Let’s write another function for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3ad4d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that apply the apply the previous merge in the splits dictionnary \n",
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
    "                split = split[:i] + [merge] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa421262",
   "metadata": {},
   "source": [
    "#### complexity of merge_pair\n",
    "The complexity of the merge_pair function is O(N * M), where N is the number of words in the corpus and M is the maximum number of subword pieces (subtokens) in a single word.\n",
    "\n",
    "Here's a breakdown of the complexity:\n",
    "\n",
    "- The function iterates over each word in the corpus exactly once (O(N)).\n",
    "- For each word, it searches through the subword pieces (subtokens) within that word to find and merge the pair (a, b).\n",
    "\n",
    "In the worst-case scenario, where every word in the corpus contains the pair (a, b), the complexity is O(N * M), where N is the number of words and M is the maximum number of subtokens in a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb49a027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ex', '##c', '##e', '##p', '##t', '##i', '##o', '##n']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#And we can have a look at the result of the first merge:\n",
    "splits = merge_pair(\"e\", \"##x\", splits)\n",
    "splits[\"exception\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fc88953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1dc4c4",
   "metadata": {},
   "source": [
    "Now we have everything we need to loop until we have learned all the merges we want. Let’s aim for a vocab size of 70:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "765d7460",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 70\n",
    "while len(vocab) < vocab_size:\n",
    "    scores = compute_pair_scores(splits)\n",
    "    best_pair, max_score = \"\", None\n",
    "    for pair, score in scores.items():\n",
    "        if max_score is None or max_score < score:\n",
    "            best_pair = pair\n",
    "            max_score = score\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    new_token = (\n",
    "        best_pair[0] + best_pair[1][2:] if best_pair[1].startswith(\"##\")\n",
    "        else best_pair[0] + best_pair[1]\n",
    "    )\n",
    "    vocab.append(new_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f55eabd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##j', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##x', '##y', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'ex', 'kw', 'tw', 'ob', 'obj', 'dj', 'exc', '##ck', 'py', 'sh', 'wh', '##cks', '##cksd', 'im', 'imp', '##mm', '##mb', '##mbd', '##mmu']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685e43e2",
   "metadata": {},
   "source": [
    "To tokenize a new text, we pre-tokenize it, split it, then apply the tokenization algorithm on each word. That is, we look for the biggest subword starting at the beginning of the first word and split it, then we repeat the process on the second part, and so on for the rest of that word and the following words in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63b93c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word(word):\n",
    "    tokens = []\n",
    "    while len(word) > 0:\n",
    "        i = len(word)\n",
    "        while i > 0 and word[:i] not in vocab:\n",
    "            i -= 1\n",
    "        if i == 0:\n",
    "            return [\"[UNK]\"]\n",
    "        tokens.append(word[:i])\n",
    "        word = word[i:]\n",
    "        if len(word) > 0:\n",
    "            word = f\"##{word}\"\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c59d8be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exc', '##e', '##p', '##t', '##i', '##o', '##n']\n",
      "['ex', '##e', '##r', '##c', '##i', '##c', '##e']\n",
      "['py', '##t', '##h', '##o', '##n']\n",
      "['[UNK]']\n"
     ]
    }
   ],
   "source": [
    "print(encode_word(\"exception\"))\n",
    "print(encode_word(\"exercice\"))\n",
    "print(encode_word(\"python\"))\n",
    "print(encode_word(\"tokenize\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d416042",
   "metadata": {},
   "source": [
    "Now, let’s write a function that tokenizes a text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b5e056bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    encoded_words = [encode_word(word) for word in pre_tokenized_text]\n",
    "    return sum(encoded_words, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8fb0543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]',\n",
       " 'i',\n",
       " '##s',\n",
       " '[UNK]',\n",
       " 's',\n",
       " '##i',\n",
       " '##m',\n",
       " '##p',\n",
       " '##l',\n",
       " '##e',\n",
       " 'd',\n",
       " '##e',\n",
       " '##m',\n",
       " '##o',\n",
       " 'o',\n",
       " '##f',\n",
       " '[UNK]',\n",
       " '[UNK]',\n",
       " 'u',\n",
       " '##s',\n",
       " '##i',\n",
       " '##n',\n",
       " '##g',\n",
       " 'py',\n",
       " '##t',\n",
       " '##h',\n",
       " '##o',\n",
       " '##n']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"This is a simple demo of WordPiece tokenization using python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39eaac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
