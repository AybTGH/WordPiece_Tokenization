## WordPiece tokenization

WordPiece is the tokenization algorithm Google developed to pretrain BERT. It has since been reused in quite a few Transformer models based on BERT, such as DistilBERT, MobileBERT, Funnel Transformers, and MPNET. Itâ€™s very similar to BPE in terms of the training, but the actual tokenization is done differently.



# Instructions

To be able to run all the commands in your local machine start by creating a virtual env by runing : python -m venv .venv

- To activate the virtual environment: `./console.bat`.
- To install dependencies: `./install.bat`
- To register added dependencies: `./freeze.bat`